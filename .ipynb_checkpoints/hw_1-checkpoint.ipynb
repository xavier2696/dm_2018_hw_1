{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name: Kelvin \n",
    "\n",
    "Student ID: 107065424\n",
    "\n",
    "GitHub ID: xavier2696"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First, you should attempt the **take home** exercises provided in the [notebook](https://github.com/omarsar/data_mining_lab/blob/master/news_data_mining.ipynb) we used for the first lab session. Attempt all the exercises, as it is counts towards the final grade of your first assignment (20%). \n",
    "\n",
    "- Then, download the dataset provided in this [link](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences#). The sentiment dataset contains a `sentence` and `score` label. Read the specificiations of the dataset before you start exploring it. \n",
    "\n",
    "\n",
    "- Then, you are asked to apply each of the data exploration and data operation steps learned in the [first lab session](https://github.com/omarsar/data_mining_lab) on **the new dataset**. You don't need to explain all the procedures as we did in the notebook, but you are expected to provide some **minimal comments** explaining your code. You are also expected to use the same libraries used in the first lab session. You are allowed to use and modify the `helper` functions we provided in the first lab session or create your own. Also, be aware that the helper functions may need modification as you are dealing with a completely different dataset. This part is worth 30% of your grade!\n",
    "\n",
    "- In addition to applying the same operations from the first lab, we are asking that you attempt the following tasks on the new sentiment dataset as well (40%):\n",
    "    - Use your creativity and imagination to generate **new data visualizations**. Refer to online resources and the Data Mining textbook for inspiration and ideas. \n",
    "    - Generate **TF-IDF features** from the tokens of each text. Refer to this Sciki-learn [guide](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) on how you may go about doing this. Keep in mind that you are generating a matrix similar to the term-document matrix we implemented in our first lab session. However, the weights will be computed differently and should represent the TF-IDF value of each word per document as opposed to the word frequency.\n",
    "    - Using both the TF-IDF and word frequency features, try to compute the **similarity** between random sentences and report results. Read the \"distance simiilarity\" section of the Data Mining textbook on what measures you can use here. [Cosine similarity](https://jamesmccaffrey.wordpress.com/2017/03/29/the-cosine-similarity-of-two-sentences/) is one of these methods but there are others. Try to explore a few of them in this exercise and report the differences in result. \n",
    "    - Lastly, implement a simple **Naive Bayes classifier** that automatically classifies the records into their categories. Try to implement this using scikit-learn built in classifiers and use both the TF-IDF features and word frequency features to build two seperate classifiers. Refer to this [nice article](https://hub.packtpub.com/implementing-3-naive-bayes-classifiers-in-scikit-learn/) on how to build this type of classifier using scikit-learn. Report the classification accuracy of both your models. If you are struggling with this step please reach us on Slack as soon as possible.   \n",
    "\n",
    "\n",
    "- Presentation matters! You are also expected to **tidy up your notebook** and attempt new data operations and techniques that you have learned so far in the Data Mining course. Surprise us! This segment is worth 10% of your grade. The idea of this exercise is to begin thinking of how you will program the concepts you have learned and the process that is involved. \n",
    "\n",
    "\n",
    "- After completing all the above tasks, you are free to remove this header block and **submit** your assignment following the guide provided in the [README.md](https://github.com/omarsar/dm_2018_hw_1/blob/master/README.md) file of the assignment's repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin Assignment Here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import helpers.data_mining_helpers as dmh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data into python array\n",
    "sentiment_data_array = []\n",
    "with open(\"sentiment_labelled_sentences_data/amazon_cells_labelled.txt\",\"r\") as amazon_data:\n",
    "    sentiment_data_array += amazon_data.read().split('\\n')\n",
    "with open(\"sentiment_labelled_sentences_data/imdb_labelled.txt\",\"r\") as imdb_data:\n",
    "    sentiment_data_array += imdb_data.read().split('\\n')\n",
    "with open(\"sentiment_labelled_sentences_data/yelp_labelled.txt\",\"r\") as yelp_data:\n",
    "    sentiment_data_array += yelp_data.read().split('\\n')\n",
    "\n",
    "sentiment_data = dmh.sentiment_data_dictionary(sentiment_data_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  score                                           sentence\n",
      "0     0  So there is no way for me to plug it in here i...\n",
      "1     1                        Good case, Excellent value.\n",
      "2     1                             Great for the jawbone.\n",
      "3     0  Tied to charger for conversations lasting more...\n",
      "4     1                                  The mic is great.\n",
      "5     0  I have to jiggle the plug to get it to line up...\n",
      "6     0  If you have several dozen or several hundred c...\n",
      "7     1        If you are Razr owner...you must have this!\n",
      "8     0                Needless to say, I wasted my money.\n",
      "9     0                   What a waste of money and time!.\n"
     ]
    }
   ],
   "source": [
    "# create dataframe\n",
    "sentiment_data_df = pd.DataFrame.from_records(data = {\"sentence\":sentiment_data['sentences'], \"score\":sentiment_data['scores']})\n",
    "print(sentiment_data_df[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
