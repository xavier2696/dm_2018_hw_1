{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name: Kelvin Xavier Munguia Velez\n",
    "\n",
    "Student ID: 107065424\n",
    "\n",
    "GitHub ID: xavier2696"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First, you should attempt the **take home** exercises provided in the [notebook](https://github.com/omarsar/data_mining_lab/blob/master/news_data_mining.ipynb) we used for the first lab session. Attempt all the exercises, as it is counts towards the final grade of your first assignment (20%). \n",
    "\n",
    "- Then, download the dataset provided in this [link](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences#). The sentiment dataset contains a `sentence` and `score` label. Read the specificiations of the dataset before you start exploring it. \n",
    "\n",
    "\n",
    "- Then, you are asked to apply each of the data exploration and data operation steps learned in the [first lab session](https://github.com/omarsar/data_mining_lab) on **the new dataset**. You don't need to explain all the procedures as we did in the notebook, but you are expected to provide some **minimal comments** explaining your code. You are also expected to use the same libraries used in the first lab session. You are allowed to use and modify the `helper` functions we provided in the first lab session or create your own. Also, be aware that the helper functions may need modification as you are dealing with a completely different dataset. This part is worth 30% of your grade!\n",
    "\n",
    "- In addition to applying the same operations from the first lab, we are asking that you attempt the following tasks on the new sentiment dataset as well (40%):\n",
    "    - Use your creativity and imagination to generate **new data visualizations**. Refer to online resources and the Data Mining textbook for inspiration and ideas. \n",
    "    - Generate **TF-IDF features** from the tokens of each text. Refer to this Sciki-learn [guide](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) on how you may go about doing this. Keep in mind that you are generating a matrix similar to the term-document matrix we implemented in our first lab session. However, the weights will be computed differently and should represent the TF-IDF value of each word per document as opposed to the word frequency.\n",
    "    - Using both the TF-IDF and word frequency features, try to compute the **similarity** between random sentences and report results. Read the \"distance simiilarity\" section of the Data Mining textbook on what measures you can use here. [Cosine similarity](https://jamesmccaffrey.wordpress.com/2017/03/29/the-cosine-similarity-of-two-sentences/) is one of these methods but there are others. Try to explore a few of them in this exercise and report the differences in result. \n",
    "    - Lastly, implement a simple **Naive Bayes classifier** that automatically classifies the records into their categories. Try to implement this using scikit-learn built in classifiers and use both the TF-IDF features and word frequency features to build two seperate classifiers. Refer to this [nice article](https://hub.packtpub.com/implementing-3-naive-bayes-classifiers-in-scikit-learn/) on how to build this type of classifier using scikit-learn. Report the classification accuracy of both your models. If you are struggling with this step please reach us on Slack as soon as possible.   \n",
    "\n",
    "\n",
    "- Presentation matters! You are also expected to **tidy up your notebook** and attempt new data operations and techniques that you have learned so far in the Data Mining course. Surprise us! This segment is worth 10% of your grade. The idea of this exercise is to begin thinking of how you will program the concepts you have learned and the process that is involved. \n",
    "\n",
    "\n",
    "- After completing all the above tasks, you are free to remove this header block and **submit** your assignment following the guide provided in the [README.md](https://github.com/omarsar/dm_2018_hw_1/blob/master/README.md) file of the assignment's repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin Assignment Here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\xavie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#imports and setup\n",
    "import pandas as pd\n",
    "import helpers.data_mining_helpers as dmh\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits import mplot3d\n",
    "import plotly\n",
    "plotly.tools.set_credentials_file(username='xavier2696', api_key='97cuXMXYTejnMxo9gpIk')\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools\n",
    "from sklearn import preprocessing, metrics, decomposition, pipeline, dummy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats.stats import pearsonr\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data into python array\n",
    "sentiment_data_array = []\n",
    "with open(\"sentiment_labelled_sentences_data/amazon_cells_labelled.txt\",\"r\") as amazon_data:\n",
    "    sentiment_data_array += [string + '\\tamazon' for string in amazon_data.read().split('\\n')]\n",
    "with open(\"sentiment_labelled_sentences_data/imdb_labelled.txt\",\"r\") as imdb_data:\n",
    "    sentiment_data_array += [string + '\\timdb' for string in imdb_data.read().split('\\n')]\n",
    "with open(\"sentiment_labelled_sentences_data/yelp_labelled.txt\",\"r\") as yelp_data:\n",
    "    sentiment_data_array += [string + '\\tyelp' for string in yelp_data.read().split('\\n')]\n",
    "\n",
    "sentiment_data = dmh.sentiment_data_dictionary(sentiment_data_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  score                                           sentence  source\n",
      "0     0  So there is no way for me to plug it in here i...  amazon\n",
      "1     1                        Good case, Excellent value.  amazon\n",
      "2     1                             Great for the jawbone.  amazon\n",
      "3     0  Tied to charger for conversations lasting more...  amazon\n",
      "4     1                                  The mic is great.  amazon\n",
      "5     0  I have to jiggle the plug to get it to line up...  amazon\n",
      "6     0  If you have several dozen or several hundred c...  amazon\n",
      "7     1        If you are Razr owner...you must have this!  amazon\n",
      "8     0                Needless to say, I wasted my money.  amazon\n",
      "9     0                   What a waste of money and time!.  amazon\n"
     ]
    }
   ],
   "source": [
    "# create dataframe\n",
    "sentiment_data_df = pd.DataFrame.from_records(data = {\"sentence\":sentiment_data['sentences'], \"score\":sentiment_data['scores'], \"source\":sentiment_data['sources']})\n",
    "print(sentiment_data_df[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "score       The amoung of missing records is: 0\n",
       "sentence    The amoung of missing records is: 0\n",
       "source      The amoung of missing records is: 0\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data exploration and data operation\n",
    "#check for missing values\n",
    "sentiment_data_df.isnull().apply(lambda x: dmh.check_missing_values(x))\n",
    "#there are no missing values in the dataset, the sentiment_data_dictionary method in the helpers file will\n",
    "#ignore any rows that contain either a sentence none value or a score none value\n",
    "\n",
    "#in this case I think it is better to ignore the rows that contain a none value because there is no way to generate\n",
    "#a sentence if it is missing and if we try to estimate a 0 or 1 value for the score it will just contaminate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before cleaning: 3000\n",
      "Duplicated rows: 17\n",
      "Number of rows after cleaning: 2966\n"
     ]
    }
   ],
   "source": [
    "#check if there are duplicated rows and remove them\n",
    "duplicates = sum(sentiment_data_df.duplicated('sentence'))\n",
    "print('Number of rows before cleaning: %d' % len(sentiment_data_df))\n",
    "print('Duplicated rows: %d' % duplicates)\n",
    "if duplicates > 0:\n",
    "    sentiment_data_df.drop_duplicates(keep=False, inplace=True)\n",
    "sentiment_data_df.reset_index(drop=True, inplace=True) #necessary because I discovered that after droping the duplicates the index\n",
    "#keeps the old original values and when trying to access values with [] the wrong values were being returned\n",
    "print('Number of rows after cleaning: %d' % len(sentiment_data_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sampling\n",
    "n = 500\n",
    "sentiment_data_sample = sentiment_data_df.sample(n=n, random_state=26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AxesSubplot(0.125,0.125;0.775x0.755)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#show the sampling and actual data counts in a bar graph\n",
    "sample_counts = sentiment_data_sample.score.value_counts()\n",
    "actual_counts = sentiment_data_df.score.value_counts()\n",
    "\n",
    "combined_data_frame = pd.DataFrame({'data': actual_counts,\n",
    "                    'sample': sample_counts})\n",
    "\n",
    "print(combined_data_frame.plot.bar(title = 'Sentiment Distribution', rot = 0, fontsize = 12, figsize = (8,4), tick_label = ['negative', 'positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imdb      994\n",
      "yelp      992\n",
      "amazon    980\n",
      "Name: source, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~xavier2696/15.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#new data visualization\n",
    "#pie  chart\n",
    "print(sentiment_data_df.source.value_counts())\n",
    "trace = go.Pie(labels=sentiment_data_df.source.value_counts().index, values=sentiment_data_df.source.value_counts())\n",
    "\n",
    "py.iplot([trace], filename='basic_pie_chart')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word count in each positive sentence:  13.063829787234043\n",
      "Average word count in each negative sentence:  13.60377358490566\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~xavier2696/17.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show the relation between the word count in each sentence and what sentiment it is attached to\n",
    "sentiment_data_sample_2 = sentiment_data_df.sample(n = 100)\n",
    "x_axis_array = [\"sentence_\" + str(index) for index in sentiment_data_sample_2.index]\n",
    "word_count_trace = go.Scatter(\n",
    "    x = x_axis_array,\n",
    "    y = [len(sentence.split(' ')) for sentence in sentiment_data_sample_2.sentence],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Sentence Word Count'\n",
    ")\n",
    "         \n",
    "word_sentiment_trace = go.Scatter(\n",
    "    x = x_axis_array,\n",
    "    y = [score for score in sentiment_data_sample_2.score],\n",
    "    mode = 'lines+markers',\n",
    "    name = 'Sentence Score'\n",
    ")\n",
    "         \n",
    "scatter_plot = [word_count_trace, word_sentiment_trace]\n",
    "\n",
    "positive_sentences_array = [len(row.sentence.split(' ')) for index, row in sentiment_data_sample_2.iterrows() if row.score == '1']\n",
    "print('Average word count in each positive sentence: ', sum(positive_sentences_array)/len(positive_sentences_array))\n",
    "\n",
    "negative_sentences_array = [len(row.sentence.split(' ')) for index, row in sentiment_data_sample_2.iterrows() if row.score == '0']\n",
    "print('Average word count in each negative sentence: ', sum(negative_sentences_array)/len(negative_sentences_array))\n",
    "\n",
    "py.iplot(scatter_plot, filename='Scatter Plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after running the cell above various times I can conclude there is no visible relation between the number of words\n",
    "#in a sentence and it's score(positive or negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature creation\n",
    "#unigrams\n",
    "sentiment_data_df['unigrams'] = sentiment_data_df['sentence'].apply(lambda x: dmh.tokenize_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature subset selection\n",
    "count_vect = CountVectorizer()\n",
    "frequency_counts = count_vect.fit_transform(sentiment_data_df.sentence)\n",
    "print(\"Document Term Matrix Size:\", frequency_counts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#heaptmap visualization\n",
    "sample_count_vect = CountVectorizer()\n",
    "sample_counts = sample_count_vect.fit_transform(sentiment_data_sample.sentence)\n",
    "plot_x = [\"term_\"+str(i) for i in sample_count_vect.get_feature_names()[0:n]]\n",
    "plot_y = [\"doc_\"+ str(i) for i in list(sentiment_data_sample.index)[:n]]\n",
    "plot_z = sample_counts[0:n, 0:n].toarray()\n",
    "df_todraw = pd.DataFrame(plot_z, columns = plot_x, index = plot_y)\n",
    "plt.subplots(figsize=(18, 14))\n",
    "ax = sns.heatmap(df_todraw,\n",
    "                 cmap=\"PuRd\",\n",
    "                 vmin=0, annot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dimensionality reduction\n",
    "#2 dimension PCA\n",
    "colors = ['coral', 'blue']\n",
    "scores = ['0','1']\n",
    "\n",
    "sentiment_data_reduced2 = PCA(n_components = 2).fit_transform(frequency_counts.toarray())\n",
    "\n",
    "fig = plt.figure(figsize = (25,10))\n",
    "ax = fig.subplots()\n",
    "\n",
    "for c, score in zip(colors, scores):\n",
    "    xs = sentiment_data_reduced2[sentiment_data_df['score'] == score].T[0]\n",
    "    ys = sentiment_data_reduced2[sentiment_data_df['score'] == score].T[1]\n",
    "   \n",
    "    ax.scatter(xs, ys, c = c, marker='o')\n",
    "\n",
    "ax.grid(color='gray', linestyle=':', linewidth=2, alpha=0.2)\n",
    "ax.set_xlabel('\\nX Label')\n",
    "ax.set_ylabel('\\nY Label')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 dimension PCA\n",
    "sentiment_data_reduced3 = PCA(n_components = 3).fit_transform(frequency_counts.toarray())\n",
    "fig = plt.figure(figsize = (25,10))\n",
    "\n",
    "ax1 = fig.add_subplot(1,1,1, projection='3d')\n",
    "\n",
    "for c, score in zip(colors, scores):\n",
    "    xs = sentiment_data_reduced3[sentiment_data_df['score'] == score].T[0]\n",
    "    ys = sentiment_data_reduced3[sentiment_data_df['score'] == score].T[1]\n",
    "    zs = sentiment_data_reduced3[sentiment_data_df['score'] == score].T[2]\n",
    "    \n",
    "    ax1.scatter3D(xs, ys, zs, c= c, marker = 'o')\n",
    "\n",
    "ax1.grid(color='gray', linestyle=':', linewidth=2, alpha=0.2)\n",
    "ax1.set_xlabel('\\nX Label')\n",
    "ax1.set_ylabel('\\nY Label')\n",
    "ax1.set_zlabel('\\nZ Label')\n",
    "ax1.view_init(25, 45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attribute transformation/aggregation\n",
    "#create a term frequencies array\n",
    "term_frequencies = np.asarray(frequency_counts.sum(axis=0))[0]\n",
    "\n",
    "#term frequencies visualization\n",
    "data = [go.Bar(\n",
    "            x=[\"term_\"+str(i) for i in count_vect.get_feature_names()],\n",
    "            y=term_frequencies\n",
    "    )]\n",
    "\n",
    "py.iplot(data, filename='Term Frequencies')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ordered term frequencies visualization\n",
    "term_frequencies_df = pd.DataFrame({'terms': count_vect.get_feature_names(), \n",
    "                                            'counts': term_frequencies})\n",
    "ordered_term_frequencies_df = term_frequencies_df.sort_values(by = 'counts', ascending = False)\n",
    "ordered_data = [go.Bar(\n",
    "            x=[\"term_\"+str(i) for i in ordered_term_frequencies_df['terms']],\n",
    "            y=ordered_term_frequencies_df['counts']\n",
    "    )]\n",
    "\n",
    "py.iplot(ordered_data, filename = 'Ordered Data Terms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#binarization\n",
    "#I will appy binarization to the source parameter because it does not make sense to apply it to the score which only has 2 \n",
    "#possible values\n",
    "mlb = preprocessing.LabelBinarizer()\n",
    "mlb.fit(sentiment_data_df.source)\n",
    "sentiment_data_df['bin_source'] = mlb.transform(sentiment_data_df['source']).tolist()\n",
    "sentiment_data_df[0:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_data_df[-9::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF\n",
    "tf_idf_vect = TfidfVectorizer()\n",
    "tf_idf_counts = tf_idf_vect.fit_transform(sentiment_data_df.sentence)\n",
    "print('First 10 Feature Names:', tf_idf_vect.get_feature_names()[0:10])\n",
    "print('TF-IDF Matrix Size:', tf_idf_counts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize the 20 terms with the highest values in the TF-IDF Matrix and compare it with the highest values in the \n",
    "#Count Frequency Matrix\n",
    "n = 20\n",
    "term_tf_idf = np.asarray(tf_idf_counts.sum(axis=0))[0]\n",
    "term_tf_idf_df = pd.DataFrame({'terms': tf_idf_vect.get_feature_names(), \n",
    "                                            'counts': term_tf_idf})\n",
    "ordered_term_tf_idf_df = term_tf_idf_df.sort_values(by = 'counts', ascending = False)\n",
    "\n",
    "ordered_tf_idf_data_sample = go.Bar(\n",
    "            x=[\"term_\"+str(i) for i in ordered_term_tf_idf_df['terms']][:n],\n",
    "            y=ordered_term_tf_idf_df['counts'][:n],\n",
    "            name = \"TF-IDF\"\n",
    "    )\n",
    "\n",
    "ordered_counts_data_sample = go.Bar(\n",
    "            x=[\"term_\"+str(i) for i in ordered_term_frequencies_df['terms'][:n]],\n",
    "            y=ordered_term_frequencies_df['counts'][:n],\n",
    "            name = \"Word Counts\"\n",
    "    )\n",
    "\n",
    "fig = tools.make_subplots(rows=1, cols=2, subplot_titles=('TF-IDF', 'Word Counts'))\n",
    "\n",
    "fig.append_trace(ordered_tf_idf_data_sample, 1, 1)\n",
    "fig.append_trace(ordered_counts_data_sample, 1, 2)\n",
    "\n",
    "py.iplot(fig, filename = 'TF-IDF Ordered Data Terms vs Counts Ordered Data Terms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distance similarity\n",
    "#according to the textbook the cosine similarity is normally used when comparing documents because of the sparsity of the \n",
    "#document term matrix, so I'll compare sentences with the cosine similarity and also compare the sentences with the \n",
    "#pearson's correlation coefficient and extended jaccard coefficient\n",
    "\n",
    "#I'll show the results of comparing 5 random sentences with the same sentiment, I'm comparing sentences with the same\n",
    "#sentiment to increase the possibility of them having words in common\n",
    "for i in range(0,5):\n",
    "    #show cosine similarity and pearson's correlation coefficient of 2 random documents with negative sentiment (score == 0)\n",
    "    r_negative_sentences = sentiment_data_df[sentiment_data_df['score'] == '0'].sample(n = 2)\n",
    "    print('Sentences:')\n",
    "    for sentence in r_negative_sentences.sentence:\n",
    "        print('-%s' % sentence)\n",
    "    index1 = r_negative_sentences.index[0]\n",
    "    index2 = r_negative_sentences.index[1]\n",
    "    \n",
    "    #obtain the rows in the count frequency matrix corresponding to those indexes\n",
    "    count_row1 = frequency_counts[index1:index1+1]\n",
    "    count_row2 = frequency_counts[index2:index2+1]\n",
    "\n",
    "    #obtain the rows in the TF-IDF matrix corresponding to those indexes\n",
    "    tf_idf_row1 = tf_idf_counts[index1:index1+1]\n",
    "    tf_idf_row2 = tf_idf_counts[index2:index2+1]\n",
    "\n",
    "    #cosine similarity\n",
    "    print(\"Cosine Similarity using term count:\",cosine_similarity(count_row1, count_row2)[0][0])\n",
    "    print(\"Cosine Similarity using TF-IDF:\",cosine_similarity(tf_idf_row1, tf_idf_row2)[0][0])\n",
    "    #Pearson's correlation coefficient\n",
    "    print(\"Pearson's correlation coefficient using term count:\",pearsonr(count_row1.toarray().ravel(),count_row2.toarray().ravel())[0])\n",
    "    print(\"Pearson's correlation coefficient using TF-IDF:\",pearsonr(tf_idf_row1.toarray().ravel(),tf_idf_row2.toarray().ravel())[0])\n",
    "    #Extended Jaccard coefficient\n",
    "    print(\"Extended Jaccard Coefficient using term count:\", dmh.extended_jaccard_coefficient(count_row1.toarray().ravel(), count_row2.toarray().ravel()))\n",
    "    print(\"Extended Jaccard Coefficient using TF-IDF:\", dmh.extended_jaccard_coefficient(tf_idf_row1.toarray().ravel(), tf_idf_row2.toarray().ravel()))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "#for most executions the different similarities will be zero(or close to zero) because there are very few words in common between the sentences\n",
    "#even in the sentences that have the same sentiment (either positive or negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if we calculate the cosine relation of 2 sentences we know have at least one word in common we can observe a non zero value\n",
    "#for both the term count vector and the Tf-IDF vector\n",
    "\n",
    "#known negative sentence indexes with common words\n",
    "index1 = 1455\n",
    "index2 = 1178\n",
    "print('Sentences:')\n",
    "print('-%s' % sentiment_data_df.iloc[index1].sentence)\n",
    "print('-%s' % sentiment_data_df.iloc[index2].sentence)\n",
    "#obtain the rows in the count frequency matrix corresponding to those indexes\n",
    "count_row1 = frequency_counts[index1:index1+1]\n",
    "count_row2 = frequency_counts[index2:index2+1]\n",
    "\n",
    "#obtain the rows in the TF-IDF matrix corresponding to those indexes\n",
    "tf_idf_row1 = tf_idf_counts[index1:index1+1]\n",
    "tf_idf_row2 = tf_idf_counts[index2:index2+1]\n",
    "\n",
    "#similarities\n",
    "print(\"Cosine Similarity of term count:\",cosine_similarity(count_row1, count_row2)[0][0])\n",
    "print(\"Cosine Similarity of TF-IDF:\",cosine_similarity(tf_idf_row1, tf_idf_row2)[0][0])\n",
    "print(\"Pearson's correlation coefficient using term count:\",pearsonr(count_row1.toarray().ravel(),count_row2.toarray().ravel())[0])\n",
    "print(\"Pearson's correlation coefficient using TD-IDF:\",pearsonr(tf_idf_row1.toarray().ravel(),tf_idf_row2.toarray().ravel())[0])\n",
    "print(\"Extended Jaccard Coefficient using term count:\", dmh.extended_jaccard_coefficient(count_row1.toarray().ravel(), count_row2.toarray().ravel()))\n",
    "print(\"Extended Jaccard Coefficient using TF-IDF:\", dmh.extended_jaccard_coefficient(tf_idf_row1.toarray().ravel(), tf_idf_row2.toarray().ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for every similarity measure used we can observe the similarity of the vectors using the TF-IDF counts is always lower to the\n",
    "#similarity of the vectors using the actual word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes classifiers\n",
    "#term frequency\n",
    "mnb_term_frequency = MultinomialNB()\n",
    "mnb_term_frequency.fit(frequency_counts, sentiment_data_df['score'].values)\n",
    "\n",
    "#TF-IDF\n",
    "mnb_tf_idf = MultinomialNB()\n",
    "mnb_tf_idf.fit(tf_idf_counts, sentiment_data_df['score'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing the accuracy\n",
    "#test with a single random sentence from the data set\n",
    "r_sentence = sentiment_data_df.sample(n = 1)\n",
    "print('Sentence:',r_sentence.iloc[0].sentence,'\\nReal Score:', r_sentence.iloc[0].score)\n",
    "r_sentence_index = r_sentence.index[0]\n",
    "print('Term Frequency MNB Prediction: ', mnb_term_frequency.predict(frequency_counts[r_sentence_index:r_sentence_index+1])[0])\n",
    "print('TF-IDF MNB Prediction: ', mnb_tf_idf.predict(tf_idf_counts[r_sentence_index:r_sentence_index+1])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain the accuracy of both models comparing the predicted values with the actual scores of all the sentences in the data set\n",
    "total_sentences = len(sentiment_data_df)\n",
    "tf_correct_prediction = 0\n",
    "tf_idf_correct_prediction = 0\n",
    "for index, row in sentiment_data_df.iterrows():\n",
    "\n",
    "    tf_prediction = mnb_term_frequency.predict(frequency_counts[index:index+1])[0]\n",
    "    if row.score == tf_prediction:\n",
    "        tf_correct_prediction+=1\n",
    "    \n",
    "    tf_idf_prediction = mnb_tf_idf.predict(tf_idf_counts[index:index+1])[0]\n",
    "    if row.score == tf_idf_prediction:\n",
    "        tf_idf_correct_prediction+=1\n",
    "    \n",
    "print('Term Frequency MNB Prediction Accuracy: %.2f%%' % ((tf_correct_prediction/total_sentences)*100))\n",
    "print('TF-IDF MNB Prediction Accuracy: %.2f%%' % ((tf_idf_correct_prediction/total_sentences)*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing with a sentences that are not in the dataset\n",
    "negative_sentence = \"the worst pizza I have ever eaten\"\n",
    "n_word_freq = count_vect.transform([negative_sentence]).toarray()\n",
    "n_word_tf_idf = tf_idf_vect.transform([negative_sentence]).toarray()\n",
    "print('Term Frequency MNB Prediction for negative sentence: ', mnb_term_frequency.predict(n_word_freq[0:1])[0])\n",
    "print('TF-IDF MNB Prediction for negative sentence: ', mnb_tf_idf.predict(n_word_tf_idf[0:1])[0])\n",
    "\n",
    "positive_sentence = \"the most delicious pizza I have tried\"\n",
    "p_word_freq = count_vect.transform([positive_sentence]).toarray()\n",
    "p_word_tf_idf = tf_idf_vect.transform([positive_sentence]).toarray()\n",
    "print('Term Frequency MNB Prediction for positive sentence: ', mnb_term_frequency.predict(p_word_freq[0:1])[0])\n",
    "print('TF-IDF MNB Prediction for positive sentence: ', mnb_tf_idf.predict(p_word_tf_idf[0:1])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
